# T√©cnicas Avanzadas de Coordinaci√≥n Multi-Agente

**Patrones acad√©micos, emergentes y especializados**

Versi√≥n: 2.0
Fecha: 2026-01-06

---

## Introducci√≥n

El documento anterior cubri√≥ las 6 t√©cnicas fundamentales. Aqu√≠ est√°n las **t√©cnicas avanzadas, acad√©micas y emergentes** que pueden no conocer muchos desarrolladores pero son muy potentes.

---

## Categor√≠as de T√©cnicas

### üêù Bio-Inspiradas (de la naturaleza)
### üéØ Basadas en Mercado/Econom√≠a
### üß† Basadas en Inteligencia Artificial
### üìê Basadas en Matem√°ticas/Teor√≠a
### üîÑ Basadas en Patrones de Concurrencia

---

## üêù T√âCNICAS BIO-INSPIRADAS

### 1. Swarm Intelligence (Inteligencia de Enjambre)

**Concepto:** M√∫ltiples agentes simples siguiendo reglas locales ‚Üí comportamiento complejo emergente

**Inspiraci√≥n:** Abejas, hormigas, p√°jaros

**C√≥mo funciona:**
```python
# Cada agente sigue reglas simples
class SwarmAgent:
    def decide_action(self):
        # Regla 1: Seguir a vecinos exitosos
        if self.see_successful_neighbor():
            self.copy_their_strategy()

        # Regla 2: Explorar si estancado
        if self.stuck():
            self.try_random_approach()

        # Regla 3: Comunicar descubrimientos
        if self.found_solution():
            self.broadcast_pheromone()

# 100 agentes simples ‚Üí soluci√≥n compleja emerge
swarm = [SwarmAgent() for _ in range(100)]
solution = swarm.solve(problem)
```

**Aplicaciones reales:**
- **Optimizaci√≥n de rutas** (TSP - Traveling Salesman)
- **Load balancing** din√°mico
- **B√∫squeda distribuida** de soluciones
- **Testing exploratorio** automatizado

**Ejemplo en desarrollo:**
```python
# Swarm de agentes explorando arquitecturas
class ArchitectureExplorer(SwarmAgent):
    def explore(self):
        # Prueba una arquitectura
        arch = self.generate_architecture()
        score = self.evaluate(arch)

        # Si es buena, deja "feromona"
        if score > threshold:
            self.pheromone[arch] = score

        # Lee feromonas de otros agentes
        best_archs = self.read_pheromones()

        # Mejora las mejores encontradas
        return self.improve(best_archs)

# 50 agentes exploran en paralelo
# Convergen a arquitectura √≥ptima
swarm = [ArchitectureExplorer() for _ in range(50)]
best_arch = run_swarm(swarm, iterations=100)
```

**Ventajas:**
‚úÖ Robusto - Si un agente falla, otros contin√∫an
‚úÖ Escalable - M√°s agentes = mejor exploraci√≥n
‚úÖ Auto-organizado - No necesitas coordinador central

**Desventajas:**
‚ùå Convergencia lenta
‚ùå Puede quedarse en √≥ptimos locales
‚ùå Dif√≠cil predecir comportamiento

**Herramientas:**
- `pyswarm` (Python)
- `swarmjs` (JavaScript)
- Custom con Claude Code + SDK

---

### 2. Ant Colony Optimization (ACO)

**Concepto:** Agentes dejan "rastros" (feromonas), otros los siguen

**Algoritmo:**
```python
class AntAgent:
    def find_path(self, start, goal):
        path = []
        current = start

        while current != goal:
            # Lee feromonas de caminos
            options = self.get_neighbors(current)
            pheromones = [self.pheromone[edge] for edge in options]

            # Elige basado en feromonas + heur√≠stica
            probs = self.calculate_probabilities(pheromones)
            next_node = random.choice(options, p=probs)

            path.append(next_node)
            current = next_node

        # Deposita feromona seg√∫n calidad del path
        quality = self.evaluate_path(path)
        for edge in path:
            self.pheromone[edge] += quality

        # Evaporaci√≥n de feromonas viejas
        self.evaporate_pheromones()

        return path

# M√∫ltiples hormigas exploran en paralelo
ants = [AntAgent() for _ in range(50)]
paths = [ant.find_path(start, goal) for ant in ants]
best_path = min(paths, key=lambda p: len(p))
```

**Aplicaci√≥n en c√≥digo:**
```python
# Encontrar secuencia √≥ptima de refactorings
class RefactoringAnt:
    def find_refactor_sequence(self, codebase):
        sequence = []
        state = codebase

        while not self.is_optimal(state):
            # Posibles refactorings
            options = self.available_refactorings(state)

            # Lee "feromonas" (qu√© refactorings funcionaron antes)
            pheromones = [self.memory[ref] for ref in options]

            # Elige refactoring
            refactoring = self.choose(options, pheromones)

            # Aplica
            state = self.apply(state, refactoring)
            sequence.append(refactoring)

        # Si secuencia es buena, refuerza memoria
        if self.quality(state) > threshold:
            for ref in sequence:
                self.memory[ref] += 1

        return sequence
```

---

### 3. Particle Swarm Optimization (PSO)

**Concepto:** Part√≠culas (agentes) se mueven por espacio de soluciones, influenciadas por:
- Su mejor posici√≥n personal
- Mejor posici√≥n global del enjambre

**Aplicaci√≥n:** Tuning de hiperpar√°metros, optimizaci√≥n

```python
class Particle:
    def __init__(self):
        self.position = random_position()
        self.velocity = random_velocity()
        self.best_position = self.position
        self.best_score = -inf

    def update(self, global_best):
        # Velocidad influenciada por:
        # - Inercia (continuar direcci√≥n)
        # - Mejor personal (volver a lo que funcion√≥)
        # - Mejor global (ir donde otros tuvieron √©xito)

        self.velocity = (
            inertia * self.velocity +
            cognitive * (self.best_position - self.position) +
            social * (global_best - self.position)
        )

        self.position += self.velocity

        # Evaluar nueva posici√≥n
        score = evaluate(self.position)
        if score > self.best_score:
            self.best_score = score
            self.best_position = self.position

# Swarm de part√≠culas
swarm = [Particle() for _ in range(100)]
for iteration in range(1000):
    global_best = max(swarm, key=lambda p: p.best_score)
    for particle in swarm:
        particle.update(global_best.best_position)
```

---

## üéØ T√âCNICAS BASADAS EN MERCADO

### 4. Auction-Based Allocation (Asignaci√≥n por Subastas)

**Concepto:** Tareas se subastan, agentes "pujan"

```python
class AuctionCoordinator:
    def allocate_tasks(self, tasks, agents):
        for task in tasks:
            # Subasta
            bids = []
            for agent in agents:
                # Agente calcula cu√°nto "paga" por hacer esta tarea
                bid = agent.calculate_bid(task)
                # Bid bajo = quiere hacerlo (tiene capacidad, skills)
                bids.append((agent, bid))

            # Ganador = bid m√°s bajo (m√°s eficiente)
            winner = min(bids, key=lambda x: x[1])

            # Asignar tarea
            winner[0].assign(task)

class Agent:
    def calculate_bid(self, task):
        # Factores:
        # - ¬øTengo las skills?
        # - ¬øCu√°nto tiempo me toma?
        # - ¬øEstoy ocupado?
        # - ¬øQu√© tan urgente?

        if not self.has_skills(task):
            return float('inf')  # No puedo hacerlo

        time_cost = self.estimate_time(task)
        load_cost = self.current_load
        urgency_bonus = task.urgency

        return time_cost * load_cost - urgency_bonus
```

**Aplicaci√≥n en desarrollo:**
```python
# Asignar m√≥dulos a desarrolladores
class DeveloperAgent:
    def __init__(self, skills, capacity):
        self.skills = skills
        self.capacity = capacity
        self.current_load = 0

    def calculate_bid(self, module):
        # Bid bajo si:
        # - Tengo skills para este m√≥dulo
        # - Tengo capacidad
        # - He trabajado en m√≥dulos similares

        skill_match = self.skill_score(module)
        if skill_match < 0.3:
            return float('inf')  # No soy bueno para esto

        time_estimate = self.estimate(module) / skill_match
        capacity_factor = 1 + self.current_load / self.capacity

        return time_estimate * capacity_factor

# Coordinator subasta m√≥dulos
coordinator = AuctionCoordinator()
modules = ["auth", "api", "frontend", "database"]
developers = [dev1, dev2, dev3]

assignments = coordinator.allocate_tasks(modules, developers)
# Resultado: Asignaci√≥n √≥ptima basada en skills y load
```

**Ventajas:**
‚úÖ Asignaci√≥n eficiente autom√°tica
‚úÖ Load balancing natural
‚úÖ Agentes con skills raras se especializan

---

### 5. Market-Based (Basado en Mercado)

**Concepto:** Oferta y demanda determinan asignaciones

```python
class MarketCoordinator:
    def __init__(self):
        self.task_prices = {}  # Precio actual de cada tipo de tarea

    def allocate(self, tasks, agents):
        for task in tasks:
            # Precio basado en oferta/demanda
            suppliers = [a for a in agents if a.can_do(task)]
            price = self.calculate_price(task, len(suppliers))

            # Agentes con menor coste de oportunidad aceptan
            for agent in suppliers:
                cost = agent.opportunity_cost(task)
                if price > cost:
                    agent.accept(task, price)
                    break

            # Actualizar precio seg√∫n escasez
            if task not in assigned:
                self.task_prices[task.type] *= 1.1  # Subir precio
```

**Ejemplo real:**
```python
# Sistema de tareas con precios din√°micos
tasks = {
    "write_tests": {"price": 10, "difficulty": 0.3},
    "implement_api": {"price": 50, "difficulty": 0.8},
    "debug_production": {"price": 100, "difficulty": 0.9}
}

# Agentes eligen seg√∫n ratio precio/dificultad
class EconomicAgent:
    def choose_task(self, available_tasks):
        # Maximizar ganancia / esfuerzo
        return max(available_tasks,
                   key=lambda t: t['price'] / t['difficulty'])
```

---

### 6. Contract Net Protocol

**Concepto:** Protocolo formal de negociaci√≥n

```
Manager ‚Üí Broadcast: "Necesito que alguien haga X"
Agents ‚Üí Reply: "Yo puedo, me cuesta Y"
Manager ‚Üí Award: "Agent 3, es tuyo"
Agent 3 ‚Üí Execute + Report
```

**Implementaci√≥n:**
```python
class ContractNetManager:
    async def assign_task(self, task):
        # 1. Announce (CFP - Call For Proposals)
        proposals = await self.broadcast_cfp(task)

        # 2. Collect proposals
        # proposals = [(agent_id, cost, time), ...]

        # 3. Evaluate proposals
        best = min(proposals, key=lambda p: p[1] * p[2])

        # 4. Award contract
        await self.award_contract(best[0], task)

        # 5. Monitor execution
        result = await self.monitor(best[0])

        return result

class ContractNetAgent:
    async def handle_cfp(self, task):
        if self.can_do(task):
            cost = self.calculate_cost(task)
            time = self.estimate_time(task)
            await self.submit_proposal(cost, time)
```

---

## üß† T√âCNICAS BASADAS EN IA

### 7. Multi-Agent Reinforcement Learning (MARL)

**Concepto:** Agentes aprenden a cooperar via reinforcement learning

```python
class RLAgent:
    def __init__(self):
        self.q_table = {}  # State-Action values
        self.epsilon = 0.1  # Exploration rate

    def choose_action(self, state, other_agents_states):
        # Estado incluye lo que hacen otros agentes
        joint_state = (state, tuple(other_agents_states))

        if random() < self.epsilon:
            # Explore
            return random_action()
        else:
            # Exploit
            return argmax(self.q_table[joint_state])

    def learn(self, state, action, reward, next_state):
        # Q-learning update
        old_value = self.q_table[(state, action)]
        next_max = max(self.q_table[next_state].values())

        new_value = old_value + alpha * (
            reward + gamma * next_max - old_value
        )

        self.q_table[(state, action)] = new_value

# Training
agents = [RLAgent() for _ in range(3)]
for episode in range(10000):
    states = env.reset()
    while not done:
        # Cada agente elige acci√≥n
        actions = [
            agent.choose_action(state, [a.state for a in agents])
            for agent, state in zip(agents, states)
        ]

        # Execute
        next_states, rewards, done = env.step(actions)

        # Learn
        for agent, s, a, r, ns in zip(agents, states, actions, rewards, next_states):
            agent.learn(s, a, r, ns)

        states = next_states
```

**Aplicaci√≥n:** Agentes aprenden a dividirse trabajo √≥ptimamente

---

### 8. Hierarchical Task Networks (HTN)

**Concepto:** Descomposici√≥n jer√°rquica de tareas

```python
class HTNPlanner:
    def decompose(self, high_level_task):
        """
        Descompone tarea en subtareas recursivamente
        """
        if self.is_primitive(high_level_task):
            return [high_level_task]

        # Encontrar m√©todo de descomposici√≥n
        method = self.find_method(high_level_task)

        # Aplicar descomposici√≥n
        subtasks = method.decompose(high_level_task)

        # Recursivamente descomponer subtareas
        primitive_tasks = []
        for subtask in subtasks:
            primitive_tasks.extend(self.decompose(subtask))

        return primitive_tasks

# Ejemplo
planner = HTNPlanner()

# Tarea de alto nivel
task = "Implementar login"

# HTN descompone:
"""
Implementar login
‚îú‚îÄ Implementar backend
‚îÇ  ‚îú‚îÄ Crear endpoint /auth/login
‚îÇ  ‚îú‚îÄ Implementar JWT
‚îÇ  ‚îî‚îÄ Hashear passwords
‚îú‚îÄ Implementar frontend
‚îÇ  ‚îú‚îÄ Crear LoginForm
‚îÇ  ‚îú‚îÄ Manejar errores
‚îÇ  ‚îî‚îÄ Guardar token
‚îî‚îÄ Implementar tests
   ‚îú‚îÄ Tests unitarios backend
   ‚îú‚îÄ Tests unitarios frontend
   ‚îî‚îÄ Tests E2E
"""

primitive_tasks = planner.decompose(task)
# Asignar primitive_tasks a agentes
```

---

### 9. BDI Agents (Belief-Desire-Intention)

**Concepto:** Agentes con modelo mental expl√≠cito

```python
class BDIAgent:
    def __init__(self):
        self.beliefs = {}     # Lo que sabe del mundo
        self.desires = []     # Sus metas
        self.intentions = []  # Planes activos

    def sense(self):
        """Actualizar creencias desde entorno"""
        new_info = self.perceive_environment()
        self.beliefs.update(new_info)

    def deliberate(self):
        """Decidir qu√© deseos perseguir"""
        # Filtrar deseos seg√∫n creencias actuales
        achievable = [d for d in self.desires
                     if self.is_achievable(d, self.beliefs)]

        # Priorizar
        self.intentions = self.prioritize(achievable)

    def plan(self):
        """Crear planes para intenciones"""
        for intention in self.intentions:
            if not self.has_plan(intention):
                plan = self.generate_plan(intention, self.beliefs)
                self.add_plan(intention, plan)

    def execute(self):
        """Ejecutar siguiente acci√≥n del plan"""
        if self.intentions:
            current_intention = self.intentions[0]
            plan = self.get_plan(current_intention)
            action = plan.next_action()
            self.perform(action)

    def run(self):
        """BDI loop"""
        while True:
            self.sense()        # Actualizar creencias
            self.deliberate()   # Elegir intenciones
            self.plan()         # Planear
            self.execute()      # Actuar
```

**Aplicaci√≥n en desarrollo:**
```python
class DeveloperBDI(BDIAgent):
    def __init__(self):
        super().__init__()

        # Creencias
        self.beliefs = {
            "codebase_state": "needs_refactoring",
            "test_coverage": 0.6,
            "deadline": "2 weeks"
        }

        # Deseos
        self.desires = [
            "increase_test_coverage",
            "refactor_legacy_code",
            "implement_new_feature"
        ]

    def deliberate(self):
        # Si deadline cercano, priorizar features
        if self.beliefs["deadline"] < "1 week":
            self.intentions = ["implement_new_feature"]
        # Si coverage bajo, priorizar tests
        elif self.beliefs["test_coverage"] < 0.7:
            self.intentions = ["increase_test_coverage"]
        else:
            self.intentions = ["refactor_legacy_code"]
```

---

## üìê T√âCNICAS MATEM√ÅTICAS

### 10. Consensus Algorithms (Raft, Paxos)

**Concepto:** M√∫ltiples agentes alcanzan acuerdo sobre un valor

**Raft (simplificado):**
```python
class RaftAgent:
    def __init__(self):
        self.state = "follower"  # follower, candidate, leader
        self.current_term = 0
        self.voted_for = None
        self.log = []

    def request_vote(self, term, candidate_id):
        """Otro agente pide tu voto"""
        if term > self.current_term and self.voted_for is None:
            self.voted_for = candidate_id
            return True
        return False

    def become_leader(self):
        """Gan√© la elecci√≥n"""
        self.state = "leader"

        # Ahora coordino a otros
        self.heartbeat_to_followers()

    def append_entries(self, entries):
        """Leader env√≠a entradas del log"""
        self.log.extend(entries)

        # Replicar a followers
        for follower in self.followers:
            follower.replicate(entries)

# Sistema distribuido
agents = [RaftAgent() for _ in range(5)]

# Uno se vuelve leader
agents[0].become_leader()

# Leader coordina decisiones
agents[0].decide("deploy_version_2.0")
# Todos los agentes replican la decisi√≥n
```

**Aplicaci√≥n:** Tomar decisiones que todos los agentes aceptan

---

### 11. Gossip Protocol (Protocolo de Chisme)

**Concepto:** Informaci√≥n se propaga peer-to-peer como rumores

```python
class GossipAgent:
    def __init__(self, peers):
        self.peers = peers
        self.knowledge = set()

    def learn(self, info):
        """Aprendo algo nuevo"""
        if info not in self.knowledge:
            self.knowledge.add(info)
            # "Chismear" a algunos peers
            self.gossip(info)

    def gossip(self, info):
        """Cuento a peers aleatorios"""
        # Seleccionar k peers aleatorios (fanout)
        targets = random.sample(self.peers, k=3)

        for peer in targets:
            peer.learn(info)  # Puede propagar m√°s

# Red de agentes
agents = [GossipAgent([...]) for _ in range(100)]

# Agente 1 aprende algo
agents[0].learn("API v2.0 ready")

# Despu√©s de k rounds, TODOS lo saben
# Propagaci√≥n exponencial: 1 ‚Üí 3 ‚Üí 9 ‚Üí 27 ‚Üí 81 ‚Üí ...
```

**Ventajas:**
‚úÖ Sin punto central de fallo
‚úÖ Escalable (log N rounds para N agentes)
‚úÖ Robusto a fallos

**Aplicaci√≥n:** Propagaci√≥n de configuraci√≥n, descubrimiento de servicios

---

### 12. Tuple Spaces (Linda)

**Concepto:** Espacio compartido de tuplas (datos estructurados)

```python
class TupleSpace:
    def __init__(self):
        self.tuples = []

    def out(self, tuple):
        """Agente escribe tupla"""
        self.tuples.append(tuple)

    def in_(self, pattern):
        """Agente lee y ELIMINA tupla que matchea"""
        for i, t in enumerate(self.tuples):
            if self.matches(t, pattern):
                return self.tuples.pop(i)
        # Bloquea hasta que aparece
        return self.wait_for(pattern)

    def rd(self, pattern):
        """Agente lee (sin eliminar)"""
        for t in self.tuples:
            if self.matches(t, pattern):
                return t
        return self.wait_for(pattern)

    def matches(self, tuple, pattern):
        """Pattern matching"""
        # pattern puede tener wildcards
        # ("task", ?, "pending") matchea ("task", "implement_api", "pending")
        pass

# Uso
space = TupleSpace()

# Agente 1: Publica tarea
space.out(("task", "implement_api", "pending"))

# Agente 2: Toma tarea
task = space.in_(("task", ?, "pending"))
# task = ("task", "implement_api", "pending")

# Agente 2: Marca como completada
space.out(("task", "implement_api", "completed"))

# Agente 3: Lee todas las completadas
completed = space.rd(("task", ?, "completed"))
```

**Ventajas:**
‚úÖ Desacoplamiento total
‚úÖ Pattern matching potente
‚úÖ Comunicaci√≥n as√≠ncrona

---

## üîÑ PATRONES DE CONCURRENCIA

### 13. Fork-Join Pattern

**Concepto:** Divide trabajo, ejecuta en paralelo, une resultados

```python
async def fork_join(task, num_workers):
    # Fork: Dividir tarea
    subtasks = task.split(num_workers)

    # Parallel execution
    results = await asyncio.gather(*[
        worker.execute(subtask)
        for worker, subtask in zip(workers, subtasks)
    ])

    # Join: Unir resultados
    final_result = task.merge(results)

    return final_result

# Ejemplo
task = "Testear 1000 endpoints"
subtasks = [
    "Testear endpoints 1-250",
    "Testear endpoints 251-500",
    "Testear endpoints 501-750",
    "Testear endpoints 751-1000"
]

results = await fork_join(task, 4)
# 4x m√°s r√°pido
```

---

### 14. Work Stealing

**Concepto:** Agentes ociosos "roban" trabajo de agentes ocupados

```python
class WorkStealingAgent:
    def __init__(self):
        self.queue = deque()  # Double-ended queue

    def push_task(self, task):
        """A√±ade tarea al final"""
        self.queue.append(task)

    def pop_task(self):
        """Toma tarea del final (LIFO local)"""
        if self.queue:
            return self.queue.pop()
        return None

    def steal_task(self, victim):
        """Roba tarea del INICIO de otro agente (FIFO remoto)"""
        if victim.queue:
            return victim.queue.popleft()
        return None

    def work(self, all_agents):
        while True:
            # Intenta trabajar en tu queue
            task = self.pop_task()

            if task:
                self.execute(task)
            else:
                # Si no tienes trabajo, roba de otro
                for victim in all_agents:
                    if victim != self:
                        stolen = self.steal_task(victim)
                        if stolen:
                            self.execute(stolen)
                            break

# Sistema
agents = [WorkStealingAgent() for _ in range(4)]

# Agente 1 tiene mucho trabajo
agents[0].queue = [task1, task2, task3, task4, task5]

# Agente 2, 3, 4 ociosos ‚Üí roban de Agente 1
# Load balancing autom√°tico
```

**Ventajas:**
‚úÖ Load balancing din√°mico y autom√°tico
‚úÖ No necesitas coordinador
‚úÖ Adaptativo a workload desigual

---

### 15. MapReduce Pattern

**Concepto:** Map (transformar), Shuffle, Reduce (agregar)

```python
async def map_reduce(data, map_fn, reduce_fn, num_workers):
    # Map phase: Procesar en paralelo
    map_results = await asyncio.gather(*[
        worker.map(chunk, map_fn)
        for worker, chunk in zip(workers, split(data, num_workers))
    ])

    # Shuffle phase: Agrupar por key
    shuffled = group_by_key(flatten(map_results))

    # Reduce phase: Agregar
    reduce_results = await asyncio.gather(*[
        worker.reduce(group, reduce_fn)
        for worker, group in zip(workers, shuffled)
    ])

    return reduce_results

# Ejemplo: Contar errores en logs
def map_fn(log_line):
    if "ERROR" in log_line:
        return ("error", 1)
    return ("ok", 1)

def reduce_fn(key, values):
    return (key, sum(values))

result = await map_reduce(
    logs,
    map_fn,
    reduce_fn,
    num_workers=10
)
# result = [("error", 523), ("ok", 9477)]
```

---

## üéì T√âCNICAS ACAD√âMICAS MAS (Multi-Agent Systems)

### 16. FIPA Protocols

**FIPA** = Foundation for Intelligent Physical Agents

**Protocolos est√°ndar:**
- Contract Net (ya mencionado)
- FIPA Request
- FIPA Query
- FIPA Subscribe

```python
class FIPAAgent:
    def request(self, target_agent, action):
        """FIPA Request Protocol"""
        msg = FIPAMessage(
            performative="request",
            sender=self.id,
            receiver=target_agent,
            content=action
        )
        response = self.send_and_wait(msg)

        if response.performative == "agree":
            # Espera resultado
            result = self.wait_for_inform(target_agent)
            return result
        elif response.performative == "refuse":
            # No puede hacerlo
            return None

    def handle_request(self, msg):
        """Manejar request"""
        if self.can_do(msg.content):
            self.reply(msg.sender, "agree")
            result = self.execute(msg.content)
            self.reply(msg.sender, "inform", result)
        else:
            self.reply(msg.sender, "refuse")
```

---

### 17. Blackboard Systems (Avanzado)

**Concepto:** Pizarra compartida + Knowledge Sources + Control

```python
class BlackboardSystem:
    def __init__(self):
        self.blackboard = {}  # Pizarra compartida
        self.knowledge_sources = []  # Agentes especializados
        self.control = Controller()  # Decide qu√© KS ejecutar

    def solve(self, problem):
        self.blackboard["problem"] = problem
        self.blackboard["solution"] = None

        while not self.blackboard["solution"]:
            # Control decide qu√© KS ejecutar
            eligible = [
                ks for ks in self.knowledge_sources
                if ks.can_contribute(self.blackboard)
            ]

            best_ks = self.control.select(eligible)

            # KS contribuye a la pizarra
            best_ks.execute(self.blackboard)

        return self.blackboard["solution"]

# Ejemplo: Diagn√≥stico de bug
class SyntaxKS(KnowledgeSource):
    def can_contribute(self, bb):
        return bb.get("syntax_checked") is None

    def execute(self, bb):
        bb["syntax_checked"] = True
        bb["syntax_errors"] = self.check_syntax(bb["code"])

class LogicKS(KnowledgeSource):
    def can_contribute(self, bb):
        return (bb.get("syntax_checked") and
                bb.get("logic_checked") is None)

    def execute(self, bb):
        bb["logic_checked"] = True
        bb["logic_errors"] = self.check_logic(bb["code"])

# Sistema
system = BlackboardSystem()
system.add_ks(SyntaxKS())
system.add_ks(LogicKS())
system.add_ks(PerformanceKS())
system.add_ks(SecurityKS())

diagnosis = system.solve({"code": buggy_code})
```

---

## üíé T√âCNICAS EMERGENTES 2024-2026

### 18. Reflexion Pattern (Self-Reflection Agents)

**Concepto:** Agente se auto-eval√∫a y mejora iterativamente

```python
class ReflexionAgent:
    def solve(self, problem, max_iterations=5):
        attempt = self.initial_solution(problem)

        for i in range(max_iterations):
            # Auto-evaluaci√≥n
            score, feedback = self.evaluate(attempt, problem)

            if score >= threshold:
                return attempt  # Suficientemente bueno

            # Reflexionar sobre qu√© fall√≥
            reflection = self.reflect(attempt, feedback)

            # Mejorar basado en reflexi√≥n
            attempt = self.improve(attempt, reflection)

        return attempt

    def reflect(self, attempt, feedback):
        """Analizar qu√© sali√≥ mal"""
        return llm.generate(f"""
        Mi soluci√≥n: {attempt}
        Feedback: {feedback}

        ¬øQu√© sali√≥ mal? ¬øC√≥mo puedo mejorarlo?
        """)

# Multi-agente con reflexi√≥n
agents = [ReflexionAgent() for _ in range(3)]

# Cada agente intenta resolver
solutions = [agent.solve(problem) for agent in agents]

# Comparten reflexiones
for agent in agents:
    agent.learn_from_others([a.reflections for a in agents])

# Intentan de nuevo con aprendizaje compartido
improved = [agent.solve(problem) for agent in agents]
```

---

### 19. Chain of Agents

**Concepto:** Cadena de agentes especializados, output de uno = input del siguiente

```python
class ChainOfAgents:
    def __init__(self, agents):
        self.agents = agents  # Lista ordenada

    async def process(self, input_data):
        data = input_data

        for agent in self.agents:
            # Output del anterior ‚Üí Input del siguiente
            data = await agent.process(data)

        return data

# Ejemplo: Desarrollo completo
chain = ChainOfAgents([
    RequirementsAgent(),   # Input: idea ‚Üí Output: requirements
    ArchitectAgent(),      # Input: requirements ‚Üí Output: architecture
    CoderAgent(),          # Input: architecture ‚Üí Output: code
    ReviewerAgent(),       # Input: code ‚Üí Output: reviewed_code
    TesterAgent(),         # Input: reviewed_code ‚Üí Output: tested_code
    DocumenterAgent()      # Input: tested_code ‚Üí Output: documented_code
])

result = await chain.process("Build a todo app")
```

---

### 20. Tree of Thoughts + Multi-Agent

**Concepto:** M√∫ltiples agentes exploran √°rbol de posibilidades

```python
class ThoughtNode:
    def __init__(self, state, parent=None):
        self.state = state
        self.parent = parent
        self.children = []
        self.value = None

class TreeOfThoughtsMultiAgent:
    def solve(self, problem, num_agents=4):
        root = ThoughtNode(problem)
        frontier = [root]

        # Agentes exploran √°rbol en paralelo
        for depth in range(max_depth):
            # Cada agente toma nodos del frontier
            agent_tasks = [
                agent.expand(node)
                for agent, node in zip(agents, frontier[:num_agents])
            ]

            new_nodes = await asyncio.gather(*agent_tasks)

            # Actualizar frontier con mejores nodos
            frontier = self.select_best(new_nodes, k=num_agents)

        # Mejor soluci√≥n encontrada
        best = max(frontier, key=lambda n: n.value)
        return self.extract_path(best)
```

---

## üéØ RECOMENDACIONES POR CASO DE USO

### Para ADD (Agent-Driven Development)

| Caso | T√©cnica Recomendada | Por Qu√© |
|------|---------------------|---------|
| Equipos peque√±os (2-5) | **Contract-First** | Simple, efectivo |
| B√∫squeda de arquitectura | **Swarm Intelligence** | Exploraci√≥n paralela |
| Asignaci√≥n din√°mica | **Auction-Based** | Auto-organizado |
| Refactoring complejo | **HTN Planning** | Descomposici√≥n jer√°rquica |
| Toma de decisiones | **Consensus (Raft)** | Acuerdo distribuido |
| Load balancing | **Work Stealing** | Autom√°tico, adaptativo |
| Testing distribuido | **MapReduce** | Paralelismo datos |
| Debugging complejo | **Blackboard System** | Conocimiento incremental |
| Iteraci√≥n de soluciones | **Reflexion** | Auto-mejora |
| Pipeline de desarrollo | **Chain of Agents** | Secuencial especializado |

---

## üî¨ INVESTIGACI√ìN Y FUTURO

### Tendencias 2026-2027

**1. LLM-Based Multi-Agent**
- Agentes powered por LLMs grandes
- Comunicaci√≥n en lenguaje natural
- "Programar" agentes con prompts

**2. Learned Coordination**
- Agentes aprenden a coordinarse (MARL)
- No necesitas programar coordinaci√≥n
- Emergen estrategias √≥ptimas

**3. Human-in-the-Loop**
- Humanos como super-agentes
- Agentes piden ayuda cuando stuck
- Colaboraci√≥n fluida

**4. Federated Multi-Agent**
- Agentes distribuidos geogr√°ficamente
- Privacy-preserving coordination
- Edge + Cloud collaboration

---

## üìö Papers y Referencias

**Cl√°sicos:**
- "Contract Net Protocol" (Smith, 1980)
- "BDI Architecture" (Rao & Georgeff, 1995)
- "Ant Colony Optimization" (Dorigo, 1992)

**Modernos:**
- "Reflexion: an autonomous agent with dynamic memory" (2023)
- "Tree of Thoughts" (2023)
- "Multi-Agent RL" (varios, 2020-2025)

**Frameworks:**
- LangGraph (LangChain)
- CrewAI
- AutoGen (Microsoft)
- MetaGPT

---

## üí° Conclusi√≥n

**Existen MUCHAS m√°s t√©cnicas** de las 6 b√°sicas que cubrimos antes.

**Para ADD 2.0, las m√°s √∫tiles son:**

1. **Contract-First** (ya incluido)
2. **Work Stealing** (load balancing autom√°tico)
3. **MapReduce** (testing paralelo)
4. **Auction-Based** (asignaci√≥n de m√≥dulos)
5. **Swarm Intelligence** (exploraci√≥n de arquitecturas)
6. **Reflexion** (agentes que se auto-mejoran)

**La m√°s simple y efectiva: Contract-First**
- Ya est√° en ADD 2.0 (fase DESIGN)
- Funciona con cualquier herramienta
- 80% del beneficio, 20% de la complejidad

**Para equipos avanzados: Contract-First + Work Stealing + Auction**
- Contract-First para independencia
- Work Stealing para balanceo
- Auction para asignaci√≥n √≥ptima

---

**Versi√≥n:** 2.0
**Autor:** H√©ctor Prats
**Fecha:** 2026-01-06

---

**¬øNecesitas profundizar en alguna t√©cnica espec√≠fica?**
